{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__‚ùóThis notebook is based on the Ethan Rosenthal's blogpost [here](https://www.ethanrosenthal.com/2015/11/02/intro-to-collaborative-filtering/).__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>881250949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>878887116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>880606923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "      <td>886397596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating  timestamp\n",
       "0      196      242       3  881250949\n",
       "1      186      302       3  891717742\n",
       "2       22      377       1  878887116\n",
       "3      244       51       2  880606923\n",
       "4      166      346       1  886397596"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH = \"../data/ml-100k/\"\n",
    "\n",
    "names = [\"user_id\", \"item_id\", \"rating\", \"timestamp\"]\n",
    "\n",
    "df = pd.read_csv(os.path.join(DATA_PATH, \"u.data\"), names=names, sep='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 943 users and 1682 items\n"
     ]
    }
   ],
   "source": [
    "n_users = df.user_id.unique().shape[0]\n",
    "n_items = df.item_id.unique().shape[0]\n",
    "\n",
    "print(f\"There are {n_users} users and {n_items} items\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make numpy matrix with ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5., 3., 4., ..., 0., 0., 0.],\n",
       "       [4., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [5., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 5., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = np.zeros((n_users, n_items))\n",
    "for _, row in df.iterrows():\n",
    "    ratings[row.user_id - 1, row.item_id - 1] = row.rating\n",
    "\n",
    "ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate sparcity of the ratings matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparcity: 6.30%\n"
     ]
    }
   ],
   "source": [
    "sparcity = np.sum(ratings != 0) / np.prod(ratings.shape) * 100\n",
    "print(\"Sparcity: {:4.2f}%\".format(sparcity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that only 6.3% of data has non-zero elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(ratings: np.ndarray):\n",
    "    test = np.zeros(ratings.shape)\n",
    "    train = ratings.copy()\n",
    "    for user in range(ratings.shape[0]):\n",
    "        test_ratings = np.random.choice(ratings[user, :].nonzero()[0], size=10, replace=False)\n",
    "        train[user, test_ratings] = 0\n",
    "        test[user, test_ratings] = ratings[user, test_ratings]\n",
    "        \n",
    "    assert np.all((train * test) == 0)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 types of collaborative filtering: user- and item-based CF. In both cases the similarity function $sim(x, y)$ is calculated which is used for selecting the most similars users/items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "sim(u,v)=\\dfrac{\\langle r_u,\\, r_v \\rangle}{\\lVert r_u \\rVert \\lVert r_v \\rVert},\n",
    "$$  \n",
    "where $r_u,\\, r_v$ are ratings of $u$ and $v$ users respectively. The same method is also working for item-item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(ratings: np.ndarray, kind=\"item\", eps=1e-6):\n",
    "    if kind == \"item\":\n",
    "        norms = np.linalg.norm(ratings, axis=0) + eps\n",
    "        return np.matmul(ratings.T, ratings) / norms / norms[None, :]\n",
    "    elif kind == \"user\":\n",
    "        norms = np.linalg.norm(ratings, axis=1) + eps\n",
    "        return np.matmul(ratings, ratings.T) / norms / norms[None, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jaccard index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This metric is often used in segmentation tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "sim(u,\\, v)= \\dfrac{|I_u \\cap I_v|}{|I_u \\cup I_v|},\n",
    "$$  \n",
    "where $I_u$ and $I_v$ are sets of rated items for users $u$ and $v$ respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(ratings: np.ndarray, kind=\"item\", eps=1e-6):\n",
    "    \n",
    "    if kind == \"item\":\n",
    "        similarity = np.zeros((ratings.shape[1], ratings.shape[1]))\n",
    "        \n",
    "        for i in range(len(similarity)):\n",
    "            for j in range(i, len(similarity)):\n",
    "                denom = (ratings[:, i] + ratings[:, j] > 0).sum() + eps\n",
    "                similarity[i][j] = (ratings[:, i] * ratings[:, j] > 0).sum() / denom\n",
    "                similarity[j][i] = similarity[i][j]\n",
    "    elif kind == \"user\":\n",
    "        similarity = np.zeros((ratings.shape[0], ratings.shape[0]))\n",
    "        \n",
    "        for i in range(len(similarity)):\n",
    "            for j in range(i, len(similarity)):\n",
    "                denom = (ratings[i] + ratings[j] > 0).sum() + eps\n",
    "                similarity[i][j] = (ratings[i] * ratings[j] > 0).sum() / denom\n",
    "                similarity[j][i] = similarity[i][j]\n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dot product similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "sim(u,\\, v) = \\langle u,\\, v \\rangle\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product_similarity(ratings: np.ndarray, kind=\"item\"):\n",
    "    if kind == \"item\":\n",
    "        return ratings.T.dot(ratings)\n",
    "    elif kind == \"user\":\n",
    "        return ratings.dot(ratings.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pearson correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_similarity(ratings: np.ndarray, kind=\"item\", eps=1e-6):\n",
    "    mask = ratings > 0\n",
    "    if kind == \"user\":\n",
    "        mean_ratings = ratings.mean(axis=1)\n",
    "        \n",
    "        ratings_centralized = (ratings - mean_ratings[:, None]) * mask\n",
    "        ratings_centralized = ratings_centralized.dot(ratings_centralized.T)\n",
    "    elif kind == \"item\":\n",
    "        mean_ratings = ratings.mean(axis=0)\n",
    "        \n",
    "        ratings_centralized = (ratings - mean_ratings[None, :]) * mask\n",
    "        ratings_centralized = ratings_centralized.T.dot(ratings_centralized)\n",
    "        \n",
    "    denom = np.linalg.norm(ratings_centralized, axis=1) + eps\n",
    "    return ratings_centralized / denom[:, None] / denom[None, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's predict item $i$ rating for user $u$:\n",
    "$$\n",
    "\\hat{r}_{ui} = \\dfrac{\\sum_{v\\in U}sim(u,\\, v)\\cdot r_{vi}}{\\sum_{v\\in U}|sim(u,\\, v)|},\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1682,)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.sum(axis=0).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(ratings: np.ndarray, similarity: np.ndarray, kind=\"item\", eps=1e-5):\n",
    "    denom = np.abs(similarity).sum(axis=1) + eps\n",
    "    if kind == \"user\":\n",
    "        assert similarity.shape[0] == ratings.shape[0], f\"{similarity.shape}, {ratings.shape}\"\n",
    "        \n",
    "        return similarity.dot(ratings) / denom[:, None]\n",
    "    elif kind == \"item\":\n",
    "        assert similarity.shape[0] == ratings.shape[1], f\"{similarity.shape}, {ratings.shape}\"\n",
    "        \n",
    "        return ratings.dot(similarity) / denom[None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.17488152, 0.6178573 , 0.94690924, 0.38254766],\n",
       "       [0.0379716 , 0.0929734 , 0.1885205 , 0.06397883],\n",
       "       [0.01276649, 0.03836561, 0.06985152, 0.02393658],\n",
       "       [0.01014075, 0.03099916, 0.05517531, 0.01924283]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_prediction = predict(train, item_similarity, kind=\"item\")\n",
    "user_prediction = predict(train, user_similarity, kind=\"user\")\n",
    "\n",
    "item_prediction[:4, :4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(prediction, true):\n",
    "    prediction = prediction[true.nonzero()].flatten()\n",
    "    true = true[true.nonzero()].flatten()\n",
    "    return np.mean((prediction - true) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-based CF MSE: 9.763106529769319\n",
      "Item-based CF MSE: 16.98220326913952\n"
     ]
    }
   ],
   "source": [
    "print(f\"User-based CF MSE: {mse(user_prediction, test)}\")\n",
    "print(f\"Item-based CF MSE: {mse(item_prediction, test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias-substracted Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As users can rate movies in different ranges, we want to track only deviation of current user's rating and his normal rating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{r}_{ui}=\\overline{r}_u + \\dfrac{\\sum_{v\\in U}sim(u,\\, v)(r_{vi} - \\overline{r}_v)}{\\sum_{v\\in U}|sim(u,\\, v)|}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1682,)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.mean(axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_substracted_predict(ratings: np.ndarray, similarity: np.ndarray, kind=\"item\", eps=1e-5):\n",
    "    denom = np.abs(similarity).sum(axis=1) + eps\n",
    "    \n",
    "    if kind == \"item\":\n",
    "        assert similarity.shape[0] == ratings.shape[1], f\"{similarity.shape}, {ratings.shape}\"\n",
    "        \n",
    "        mean_ratings = ratings.mean(axis=0)\n",
    "        \n",
    "        return (ratings - mean_ratings[None, :]).dot(similarity) / denom[None, :] + mean_ratings[None, :]\n",
    "    elif kind == \"user\":\n",
    "        assert similarity.shape[0] == ratings.shape[0], f\"{similarity.shape}, {ratings.shape}\"\n",
    "        mean_ratings = ratings.mean(axis=1)\n",
    "        \n",
    "        return similarity.dot(ratings - mean_ratings[:, None]) / denom[:, None] + mean_ratings[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.74667644, 0.82382754, 0.9039091 , 0.99671295],\n",
       "       [1.60976652, 0.29894365, 0.14552036, 0.67814412],\n",
       "       [1.58456141, 0.24433586, 0.02685137, 0.63810187],\n",
       "       [1.58193567, 0.2369694 , 0.01217516, 0.63340812]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_prediction = bias_substracted_predict(train, item_similarity, kind=\"item\")\n",
    "user_prediction = bias_substracted_predict(train, user_similarity, kind=\"user\")\n",
    "\n",
    "item_prediction[:4, :4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias-substracted User-based CF MSE: 9.56326833738522\n",
      "Bias-substracted Item-based CF MSE: 11.517464593700096\n"
     ]
    }
   ],
   "source": [
    "print(f\"Bias-substracted User-based CF MSE: {mse(user_prediction, test)}\")\n",
    "print(f\"Bias-substracted Item-based CF MSE: {mse(item_prediction, test)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "c25b2ea09d6760a58cc13d12de9c944cd1408d2c55ef8156e8102a13839ff9bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
